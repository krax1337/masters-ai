Full Transcription:
 Did you choose the Trustees searching for next year pass? Hello. Do you choose... Institute of Business all or is that their? one who has an eye to the tent maybe to the table. Oh, she did it. Yeah. Thank you. Let's do it. It's today the deadline, yeah. It's just the deadline. By the way, do we have strict deadlines for homework, guys? No. It's not strict because I don't know. I do it. It was bad. But I would suggest to find at least like 10 to 15 minutes every day how I am doing. Because when it will come at the end, it will be like everything. So at least I'm just trying to find 10 to 15 to 20 minutes doing like by small small parts. Then every day. I'm working on this course or from others. Any, I think. Other courses, I think, didn't give a homework yet. They give data processing. It gave, it gave yesterday. I think this file formats. I, yes, I miss it. I lost it. I lost it. I lost it. I lost it. I lost it. I do. I have a question. I'm a feature. If you see today, it's a goal. I'm going to introduce it quantum computers. Yeah. Our second lesson is that there is a problem with the model because they have a resource problem. What will be in the future about it? Because if we have, they have quantum computers. I mean, they have a huge resource. How they will change. Yeah. So that's really good. It's interesting. I didn't know how they work like on the very general concept of these cubids. I still, you know, I've been studying quantum physics in university, like at least one year, probably two years, I can't remember. You know, a lot of electronics works on quantum effects. So, for example, some stuff allows the electrons to jump the barrier. They, like, actually cannot jump, so they, like, tunneling through that. And no one knows, like, how it works, but it works. And we use it in TV, and it's more fun. That's it. Like some kind of, some kind of magic stuff. It's still, I think they should be some kind of scientific explanation. But I don't know, like, how these quantum computers work. I don't understand the concept of, like, something having the value of zero and one at the same time, because, like, okay, you can't imagine that, but how about the storage, right? So, if I talk about, like, HDG or SSD, any storage, so you need to write it. And you need to, like, you should be able to read it in the future. And the most fear, like, two years, I remember, like, from the community, they're guiding quantum computers. It's okay, if they are so, like, powerful, and can make, like, much more, like, I don't know, millions times, computations per second. So, what about the encryption, right? So, I would say, or any other encryption algorithm. So, basically, encryption works, like, modern encryption works on the very basic example. Like, there are, how is it called, like, one way function in maths. I don't know how it's in English, I'm not throwing your function. So, so, the way, like, it's, it's very easy to calculate this. For example, it's very easy to multiply it to prime integers. And get the result, but it's super hard to understand, like, what are the integers you multiply it, if you receive, like, this result. And a lot of encryption is based on that, and actually, a bit coin is based on that. So, Bitcoin has several, like, Bitcoin has a lot of protocols inside, like, technologies and most of them, like, top tier encryption. We have right now. So, if Google can beat that, so they can possibly, like, high-jacking, all the new bitcoins. Maybe they can rewrite the whole bit coin, like, the whole blockchain, like, in a day, and, you know, like, steal all the bitcoins from us. So, a lot of, like, military applications, like, all this encryption and networking. So, I don't know, it's technically possible that, yeah, such technology can, can appear, but, taking an account how much time and money and resources and smart people, it takes to build such thing. So, maybe the first country which builds it in a, like, who creates some kind of, like, snow crash, like, from cyberpunk books. I don't know, so, we'll see. At least, like, I know, like, from, from what I understand, like, IT, history and technology history, like, all the new, like, top tier technologies, they have, like, two applications, like, the two industries, basically, driving them fast in, like, making them broadly available, is, uh, porn industry and military. So, this is, like, two things usually pick and up the latest touch and try to get, like, money of that, and, uh, maybe one of them will pick and up, I don't know, maybe military first. So, I'm gonna see the applications of cool quantum stuff. Yeah, but, uh, from personal perspective, like, being a human, like, demolite, 80 guys, so yeah, that's interesting, but unless I cannot benefit from that, I can, like, unless I cannot, like, earn money, doing that. So, it's inevitable for me, like, the sun, the sun just can stop, I don't know, like, okay, but, example, because the Earth's Earth is close to the sun. So, the Earth can just stop rotating and it will be infinite night for me. So, I can do anything about that. I can do anything about rain. So, what I can, is just to, to buy them, umbrella, I don't know, parade, there is stuff sometimes. And in case I have the problems, or, or something, I cannot influence. I think it's good not to worry about it, because if you worry about something, you cannot change that's bad. So, you spent a lot of resources and the Earth's outcome will be nothing, like for sure. So, my plan is just read the news. I didn't follow this agenda, but don't worry about it. Okay. Yeah, let's, let's get down to business. So, let me try to open my, open our schedule. I had to reboot my PC today, so I lost this X was pretty shit. Okay, this one. Okay, so, the good news, good news, well, and track, right, and here, here we are, meeting number five. And today we're going to discuss with very PI and local installation. The bad news, I took a look at some of the Chrome works, like two or three, maybe five, but not like very very high level. I like the blog posts you guys made, so I like the pictures, and I think I will share some of my favorite pictures next time. So, I assume I will need this weekend to check them and probably to, I don't know, because I'm just going to get some marks or something like that. So, just give me some time, but I haven't found any like critical problems there. So, just remember to share the workflow, how you did that, like for the first task. So, I need to see like what prompts you've been using, and don't forget about the pictures. So, the pictures are necessary for our workpost. So, I think we're going to focus on that probably on this lecture or this one. I'm going to have time to review the homework. So, today we're going to discuss with you, SPIRP. And this is very, let me show this slide. So, here are the slides for today. So, what is the VSPIR? And why we're discussing that? Firstly, when people discuss the generative AI or AI in general, so, if I'm focused on like, LLAMs, I like the pilots, functions and stuff like that. So, everything that works with text, right? Because it has more application to the business than the work with text. But still, usually, what is being forgotten is images and audio processing, again, I'm getting the images. I can't say this like easier to monetize this. So, not to my business applications. So, I know a lot of cases where clients came to us like to the farm and asked to create something. Like, I don't know, maybe I read over a hungry of cases. And only on the several of them, we are related to image generation. But some of them, like more, more of these cases, are related to audio. And one of the usual, like, requests is, okay, guys, we have 5,000 hours of recordings of, I don't know, meetings or customers calling us for the support. So, we need to understand this data, right? So, imagine you have a first line support, like people calling, like real users calling describing the issue and the operators are just sharing. So, what should you do to solve it? And you need to control it somehow, right? So, you need to see what are the most popular questions, what are the answers. So, is the client as happy or not, right? And for this case, you need to voice processing. And processing the voice is actually very old technology. I don't remember how old it is, but like, two things here, usually working is text, speech and speech to text. And if we check, I didn't know, like, let's check, the user takes the speech, okay, it's a yes, speech now, right? I got it. Everything is a yes now. Let me check the price in, actually, the TS price in, it's sort of cheap, it's sort of cheap, okay, I think here. So, this is, okay, free tier, all right? Basically, all right, delete this one. So, speech to text. So, in order to transcript something, like, if you have a phone call, real time transcription, one dollar per hour, that's too much. Okay, by transcription. So, if I have a lot of audio calls, I didn't like one, I can post them as a batch, and this will result me in 80 stands per hour. So, not too much, actually. As usually, like Microsoft has like very complicated, very complicated stuff for billing, so it's hard to understand on my shoes, and here is like text to speech. So, basically, to to directions like from speech, you can extract text. And from text, you can generate speech, right? And this is like the standard voice. I think this standard voice become neural, like during, like last year, because previously, the head different. They had a different pricing, like the standard voice is like this robotized voice, usually here when you call a bank or something. And neural voice is like more pleasant, more realistically, more naturally, not all. So, right now, I don't think they should have some kind of like cheap voice, like this old cheap voice. Probably it's inside the Azure East, but the trick is not expensive, right? So, one billion characters is like, it's a lot. 15 dollars for business, it's like, well, not too much, I think. So, and in case we have 1000 of like phone calls, transcript, it will result us is only like less than 200 dollars. So that's that's acceptable for that. And they definitely use machine learning there, right? So because there's no way programmatic way like algorithmically to transcribe the speech. But still, the interesting thing here is it's also possible to use machine learning techniques and all this concept of talk and we've been discussing previously, right, in in working with speech in understanding this speech. And the project, the project I would like to show you this with personal. Let me find, I should be here in the notes I put. Yeah, this one. It wasn't used very similar once the chargepits here, right? So, I just tried to recognize what was the time when chargepits here at least. Okay, yeah, chat, you can see. Okay, now that's all right, 2022. Yeah, so they introduced this per couple of months before chargepits. And then what happens like, do you remember this movie, sentence floor. This one. So do you know this movie, so to speak? It's kind of like a popular from scientific perspective, but they've got the problem, right? So this is the cyber like not that cyberpunk, but probably like interesting science fiction, but take a look at their release years 1999. The problem is that in 1999, this movie gets out matrix. And that's why like everyone know about the matrix, it was like very successful movie. And this is the reason, much more less people know that this movie, that's for even exists. If they release it like a year previews before matrix or much more success or the same as we used to write. So chargepits here release November 14 and this is actually the same company, but it was not so hyped. In September as it become in November, right? So what they introduced is they actually have been working like at the same time looks like maybe in working at the visper and parallel like working with chargepits here, right? So they reused the same like not the same but similar architecture like in quarters decoders and tokens to predict the tokens, but not from text, but from audio as well. So they just take a look at the idea, right? And in the paper, so they share like this one. So they took 600, 80,000 towers of multi-lingual audio. So they sliced it in 30 seconds, like steps, not so much chunks. And they, they trained the model, they trained the model against it using the same approach they used for the GPT. So pretty the same like sliced data into some samples, tried to predict the tokens like compare with the result, learn, trained the model and so on. So and this results in a very interesting thing. Very high quality and I don't know if there are any model right now, which can be it's inequality of visper. Because once it was released, it was like very high quality they released another model. A couple of months ago, I think it's called the visper, but their detection seems to stay the same. So and one more interesting thing here is it's completely open-served. So you can download it and it doesn't require a lot of VRAM to run. So you can run it on the GPU, you can run it a bit slower on the CPU. And as far as it's like not live language model, it will perform like with acceptable acceptable speed. I've been running it at the CPU once and it works normally, so not so slow as the LLM. So we are not so interested in technical details and I don't understand them actually just to explain to you. But what we're interested in is actually interested in the result, right? So how it works and how I can use it. So how how to use it. So you just install it as a Python package, but you need the byte search. And this is like why I mentioned the Python previously. So in other words, it's a lot of locally. So you must have the Python search. And this is the common bit of like, it should be a Python. Download probably now. Come on, where's download? Get started. Oh, here, install. Okay, like if you would like to try this, right? Or if you would like to work with Python, so there's like two options here. So it doesn't matter which version you will be installing. You're going to see the same table for every time like it's a couple of years the same. So if installing it like in straightforward like recommended way, like usually it's windows, Python, architecture, people, Python and here is like could a OCPU and depending on what you will select. The URL will be different, right? So coulda is the accelerated computing framework created by Nvidia as far as remember by Nvidia. To run the computations on the GPU. And if you download and install this by touch, you will be running everything against the GPU. If you do not have GPU, right? So you can install it without like just the CPU. And it will be the same package. It will work the same, but much more slower, right? But it will still work. So you need to prior to install the Python. So first you need to Python. Next you need to select the coulda version. So usually the later the better. You just copy this and install. And actually I did that today because my Python. But not Python but visper fate fell to run today. So I've been preparing to the demo and I also have like the. Some files. So I would do files. So and the problem is why. Facing them and you may face the same. So let me. Let me show you. So the problem with Python is that. Somehow like for for for more than applications. I use non-py version to. So non-py is a framework is a library to working like with numbers in Python. And it's required by by touch. But this this by touch and it's like implementation in visper. And it doesn't work with non-py 2.0. So it requires like all the version. And that's this kind of problem you may face as well. So what I did is actually I installed. Like virtual environments. So I created a virtual Python environment and download it once again. Pytharch, visper and all the stuff. So at least now it works, right? So that's that's the problem you may face with working with any software. In Python. At least I think I should do some kind of work around but. Taking a look at how many people using the virtual environments. I think this is this is the acceptable work around. So getting back here. So what you need is just to install the library. And that's it. You can install it from source but I prefer like this lazy option. And it it also requires ffm pack. So ffm pack. This this is the most popular library to working with audio and video. So and actually this may be useful for you in future. So I know like working in development or just processing media like audio and video requires you to do. So to convey files from one form to another. Split them. I don't know change the bit rate and anything. So everything can be done with ffm pack. But ffm pack is a common line interface. Like it's a program which allows you to do everything. But without the user interface, without the UI. But you can work with it for a bit. So for example, this is how you can convert from n before to 80 like the old form of right. And this is like the the greatest software and the defast the solution right now. I think so a lot of. Audio and video converting software use ffm pack and that we could. So it will be necessary as well if you would like to start with this. Because with this we will need to transform the data from one form or two in other. So for example, in my example, I'm going to use empathy. But in some cases, it may be one form or something like that. So this will need to try to transform from one form or another. And here are the models and languages. So as far as it's machine learning model right. So you can pick up any. And getting back here. So remember we've been discussing this like model card. Quantization, distillation and so so different models right. So this is like certain billion model. This is 70 billion model the same with the speaker. So this model has 39 million parameters. It's tiny and it requires only one gigabyte of video like. And it's very fast right. So assuming the large model has like 1.5 billion parameters. It'll occupy 10 gigabytes of b-ram. And the speed is 1 like 1 x right. So this one is 10 times faster and turbo is 8 times faster. And that's a day they have large width and large width 3. And what's more interesting here is the quality. So the quality here is measured in WEAR like world error rate. So it just evaluating how much how many words like mis-producted like how many words are wrong. And you can you can see the languages here. So depending on the data set and probably the language structure. This really differs right. So I'm not surprised the Spanish is the has like less errors than any other languages because Spanish at least for my perspective I know like a couple of hours. And it looks like very easily. Like straight forward and kind of easy. And some of the languages probably lacking the presence in the internet will result in like half of the words mis-peeltic beleration. I don't know. I think I need to I know beleration language and I think I need to find some podcasts in beleration and try to feed it into the whisper and check like how it works. But this that's someone's strange because 42 out of 100 comparing to six in Ukrainian. So the lrussian Ukrainian is super similar super similar. So I can understand like 90% of Ukrainian and the crannies understand 90% of beleration probably is the problem in the data set right. Okay, but in case like in most cases we're going to work with English right. So working with this language is super cool. And common line usage is pretty easy. So let's run the let's run the example and for the example I have the. I took the matrix movie and you know like there is a scene. I just meet a stock in his morphos. But which is matrix. And just me it. Yes, he's delivering the speech about like the humans. I'm not sure you will be able to. To hear it because I'm using teams in in browser. So it's not desktop applications. So I can't share my audio. But here is like the the agents meet is interrogating morphos and he's delivering the speech like I got the ideas. So human beings are a virus and we like the agents are the cure. So I will drop you the link or you can just google it. And and consider this is I picked this this sample because that like high quality speech and no any like music and background and something like that. So let me try to run this and check how it will work. So default. Whisperer. We just need to call Whisperer and pass the parameter as the the the data file. So right now is detecting the language isn't first or the second. And the the language is detected in which. So you can do that actually. Mainly you can specify the language and here is the output right. So this is like the Smith is fine. Discussing it this is very precise actually this is very precise. I think it's like. Probably. Here we have the problems. Yeah, I think it's messing is messing something still. But I don't know what model I'm using so let me call the. The usage. Okay, so a lot of things here. So we are interested in the model. So we can specify the model from here right. So these are the available models. I don't know which one is the default maybe small or base. So I have a lot of VRAM so I can actually watch. I'm going to launch large model. So let's try with probably another model and you can also specify the language explicitly. I don't think it will help us because it automatically recognize the business English. One more thing actually very interesting thing. You can do with this for so here should be a parameter which is called. I can find it. Press. Information. Okay, model device output we're both. The should be parameter which is called something like a prompt. Yeah, this one initial prompt. So the trick is. So by default it's not but let's get back to the idea so this per. So this per works with tokens right and everything like in this architecture you can you can pass the system. So you can pass the system. And I've been working once on English. Transcription task. So we decided to make. Transcribed the English assessments in our company so there is like an interview or ask questions and the person replying and one of the idea was to calculate the. I don't know how it's called like the world's like on when the person is thinking like. Parasite was. So it's not possible to do with just plain whisper. And it's not possible but if you explicitly provide them in a prompt if you recognize them and also. A lot of a lot of times in this dialogue. They were mentioned a company name. And there's no such world in in whisper data set. So that's why. It was looking for a similar. But if you provide it in initial prompt and it will like see the same talking. This same talking combination. It will just do not search for any other alternative. So this is very powerful thing like I'm very underrated. Okay, so let's get back here and let's try the model let me try different models and we're going to see. The performance like this speed. Let's try with tiny. This model and let's check how fast until work. Pretty fast. And let me try. Turbo. Okay, pretty fast as well. I don't see a big difference in speed here, right? At least at this sample the sample is on the one minute. Okay, yeah, the final talking was took the time and let me try large. So the large should give us the the highest quality. Okay, yeah, this is the difference. So large and turbo. There's this tiny so the tiny. You can just understand what's what's going on in this discussion. So if you don't care about particular worlds, if you just would like, I don't know, to summarize it in future and make some decision like I didn't know. So the sentiment on now is something so maybe tiny is okay, but still. That's that's a lot of questions as as far as I see. But here the turbo and the large so they they work very effectively. All right, so. And once I run every model so it's getting downloaded so I don't think I started small. So probably it will be downloading right now. No, no, it's already downloaded. Yeah, so once I selected large first time. Yeah, it started downloading all of this model like one and a half gigabytes of model locally. So if you have a lot of files, I didn't know million of hours. So what do you just need? I just need a PC. So you can use it in cloud, right? So let's check the pricing open the account. API pricing. So you can do that with API. Let me scroll to the whisper. By continuing create a system. Okay images, audio models. Okay. So what we're going to pay. We're going to pay. Zero 16 cents per minute. So let's let me calculate one hour. Okay. So zero six cents per minute. So it means one hour will cost us 36 cents. Yeah, so it means like 100 hours of transcription will result in 36 dollars. So they should be some point where it's cheaper to purchase hardware and run it on your hardware. And save the hardware for yourself instead of sending to the open. Yeah, right? Yeah. So in some cases it's more effective to do it on your on the sheen. I didn't like spend the night. Recreate some Python scripts and yeah, regarding Python, you can call it from Python as well. So it's Python library. So you can you can use it from common line, but you can also. Here is the common time usage. It can also impart it. Load the model and transcribe your own data just as passing as reference to the. To the file right and here are the examples how we can do that with. We spite them still it has one problem, which probably being solved in some forks of the whisper like whisper X and so on. The problem is it doesn't differentiate speakers so you will not see the difference between speaker one and speaker to write. So what I did in homework, I shared the file with you where there is a difference between speaker one. And speaker so here is no no any difference, but the output form us may be different right so. Let me call the help once again. Here output format you can see we have a lot of different output form was TXT with TSNT TSVG. So all let's try let's try to run with all but probably all is a default right so let me share. Okay yeah all is a default it means I already have all this information available. So let's let's check it out. If you like json's you can run this. It would be for my good right.

Segments in 30-second intervals:
[0.00s -> 36.40s]   Did you choose the Trustees searching for next year pass?  Hello.  Do you choose...  Institute of Business all or is that their?  one who has an eye to the tent maybe to the table.
[36.40s -> 72.40s]  Oh, she did it.  Yeah.  Thank you.  Let's do it.  It's today the deadline, yeah.  It's just the deadline.  By the way, do we have strict deadlines for homework, guys?  No.  It's not strict because I don't know.  I do it.  It was bad.  But I would suggest to find at least like 10 to 15 minutes every day how I am doing.
[72.40s -> 105.40s]  Because when it will come at the end, it will be like everything.  So at least I'm just trying to find 10 to 15 to 20 minutes doing like by small small parts.  Then every day.  I'm working on this course or from others.  Any, I think.  Other courses, I think, didn't give a homework yet.  They give data processing.
[105.40s -> 135.40s]  It gave, it gave yesterday.  I think this file formats.  I, yes, I miss it.  I lost it.  I lost it.  I lost it.  I lost it.  I lost it.  I do.  I have a question.  I'm a feature.
[135.40s -> 165.40s]  If you see today, it's a goal.  I'm going to introduce it quantum computers.  Yeah.  Our second lesson is that there is a problem with the model because they have a resource problem.  What will be in the future about it?  Because if we have, they have quantum computers.  I mean, they have a huge resource.  How they will change.
[165.40s -> 202.40s]  Yeah.  So that's really good.  It's interesting.  I didn't know how they work like on the very general concept of these cubids.  I still, you know, I've been studying quantum physics in university,  like at least one year, probably two years, I can't remember.  You know, a lot of electronics works on quantum effects.  So, for example, some stuff allows the electrons to jump the barrier.
[202.40s -> 232.40s]  They, like, actually cannot jump, so they, like, tunneling through that.  And no one knows, like, how it works, but it works.  And we use it in TV, and it's more fun.  That's it.  Like some kind of, some kind of magic stuff.  It's still, I think they should be some kind of scientific explanation.  But I don't know, like, how these quantum computers work.  I don't understand the concept of, like, something having the value of zero and one at the same time,
[232.40s -> 265.40s]  because, like, okay, you can't imagine that, but how about the storage, right?  So, if I talk about, like, HDG or SSD, any storage, so you need to write it.  And you need to, like, you should be able to read it in the future.  And the most fear, like, two years, I remember, like, from the community,  they're guiding quantum computers.  It's okay, if they are so, like, powerful, and can make, like, much more, like,  I don't know, millions times, computations per second.
[265.40s -> 296.40s]  So, what about the encryption, right?  So, I would say, or any other encryption algorithm.  So, basically, encryption works, like, modern encryption works on the very basic example.  Like, there are, how is it called, like, one way function in maths.  I don't know how it's in English, I'm not throwing your function.  So, so, the way, like, it's, it's very easy to calculate this.
[296.40s -> 329.40s]  For example, it's very easy to multiply it to prime integers.  And get the result, but it's super hard to understand, like, what are the integers you multiply it,  if you receive, like, this result.  And a lot of encryption is based on that, and actually, a bit coin is based on that.  So, Bitcoin has several, like, Bitcoin has a lot of protocols inside, like,  technologies and most of them, like, top tier encryption.
[329.40s -> 368.40s]  We have right now. So, if Google can beat that, so they can possibly, like,  high-jacking, all the new bitcoins.  Maybe they can rewrite the whole bit coin, like, the whole blockchain, like, in a day,  and, you know, like, steal all the bitcoins from us.  So, a lot of, like, military applications, like, all this encryption and networking.  So, I don't know, it's technically possible that, yeah, such technology can,  can appear, but, taking an account how much time and money and resources and smart people,
[368.40s -> 399.40s]  it takes to build such thing.  So, maybe the first country which builds it in a, like, who creates some kind of, like,  snow crash, like, from cyberpunk books.  I don't know, so, we'll see.  At least, like, I know, like, from, from what I understand, like,  IT, history and technology history, like, all the new, like, top tier technologies,  they have, like, two applications, like, the two industries, basically,
[399.40s -> 431.40s]  driving them fast in, like, making them broadly available,  is, uh, porn industry and military.  So, this is, like, two things usually pick and up the latest touch and try to get, like,  money of that, and, uh, maybe one of them will pick and up, I don't know, maybe military first.  So, I'm gonna see the applications of cool quantum stuff.
[431.40s -> 461.40s]  Yeah, but, uh, from personal perspective, like, being a human, like,  demolite, 80 guys, so yeah, that's interesting, but unless I cannot benefit from that,  I can, like, unless I cannot, like, earn money, doing that.  So, it's inevitable for me, like, the sun, the sun just can stop, I don't know, like, okay,  but, example, because the Earth's Earth is close to the sun.  So, the Earth can just stop rotating and it will be infinite night for me.
[461.40s -> 491.40s]  So, I can do anything about that.  I can do anything about rain.  So, what I can, is just to, to buy them, umbrella,  I don't know, parade, there is stuff sometimes.  And in case I have the problems, or, or something, I cannot influence.  I think it's good not to worry about it, because if you worry about something,  you cannot change that's bad.  So, you spent a lot of resources and the Earth's outcome will be nothing, like for sure.
[491.40s -> 523.40s]  So, my plan is just read the news.  I didn't follow this agenda, but don't worry about it.  Okay.  Yeah, let's, let's get down to business.  So, let me try to open my, open our schedule.  I had to reboot my PC today, so I lost this X was pretty shit.  Okay, this one.
[523.40s -> 556.40s]  Okay, so, the good news, good news, well, and track, right, and here, here we are, meeting number five.  And today we're going to discuss with very PI and local installation.  The bad news, I took a look at some of the Chrome works, like two or three, maybe five,  but not like very very high level.  I like the blog posts you guys made, so I like the pictures, and I think I will share some of my favorite pictures next time.
[556.40s -> 588.40s]  So, I assume I will need this weekend to check them and probably to, I don't know,  because I'm just going to get some marks or something like that.  So, just give me some time, but I haven't found any like critical problems there.  So, just remember to share the workflow, how you did that, like for the first task.  So, I need to see like what prompts you've been using, and don't forget about the pictures.  So, the pictures are necessary for our workpost.
[588.40s -> 622.40s]  So, I think we're going to focus on that probably on this lecture or this one.  I'm going to have time to review the homework.  So, today we're going to discuss with you, SPIRP.  And this is very, let me show this slide. So, here are the slides for today.  So, what is the VSPIR? And why we're discussing that?  Firstly, when people discuss the generative AI or AI in general, so, if I'm focused on like, LLAMs,
[622.40s -> 661.40s]  I like the pilots, functions and stuff like that.  So, everything that works with text, right? Because it has more application to the business than the work with text.  But still, usually, what is being forgotten is images and audio processing, again,  I'm getting the images. I can't say this like easier to monetize this.  So, not to my business applications. So, I know a lot of cases where clients came to us like to the farm and asked to create something.
[661.40s -> 691.40s]  Like, I don't know, maybe I read over a hungry of cases.  And only on the several of them, we are related to image generation. But some of them, like more, more of these cases, are related to audio.  And one of the usual, like, requests is, okay, guys, we have 5,000 hours of recordings of, I don't know, meetings or customers calling us for the support.
[691.40s -> 725.40s]  So, we need to understand this data, right? So, imagine you have a first line support, like people calling, like real users calling describing the issue and the operators are just sharing.  So, what should you do to solve it? And you need to control it somehow, right? So, you need to see what are the most popular questions, what are the answers.  So, is the client as happy or not, right? And for this case, you need to voice processing. And processing the voice is actually very old technology.
[725.40s -> 764.40s]  I don't remember how old it is, but like, two things here, usually working is text, speech and speech to text. And if we check, I didn't know, like, let's check,  the user takes the speech, okay, it's a yes, speech now, right? I got it. Everything is a yes now.  Let me check the price in, actually, the TS price in, it's sort of cheap, it's sort of cheap, okay, I think here.
[764.40s -> 802.40s]  So, this is, okay, free tier, all right? Basically, all right, delete this one. So, speech to text. So, in order to transcript something, like, if you have a phone call, real time transcription, one dollar per hour, that's too much.  Okay, by transcription. So, if I have a lot of audio calls, I didn't like one, I can post them as a batch, and this will result me in 80 stands per hour. So, not too much, actually.
[802.40s -> 839.40s]  As usually, like Microsoft has like very complicated, very complicated stuff for billing, so it's hard to understand on my shoes, and here is like text to speech. So, basically, to to directions like from speech, you can extract text.  And from text, you can generate speech, right? And this is like the standard voice. I think this standard voice become neural, like during, like last year, because previously, the head different.
[839.40s -> 871.40s]  They had a different pricing, like the standard voice is like this robotized voice, usually here when you call a bank or something. And neural voice is like more pleasant, more realistically, more naturally, not all.  So, right now, I don't think they should have some kind of like cheap voice, like this old cheap voice. Probably it's inside the Azure East, but the trick is not expensive, right? So, one billion characters is like, it's a lot.
[871.40s -> 903.40s]  15 dollars for business, it's like, well, not too much, I think. So, and in case we have 1000 of like phone calls, transcript, it will result us is only like less than 200 dollars. So that's that's acceptable for that.  And they definitely use machine learning there, right? So because there's no way programmatic way like algorithmically to transcribe the speech.
[903.40s -> 935.40s]  But still, the interesting thing here is it's also possible to use machine learning techniques and all this concept of talk and we've been discussing previously, right, in in working with speech in understanding this speech.  And the project, the project I would like to show you this with personal.  Let me find, I should be here in the notes I put. Yeah, this one.
[935.40s -> 968.40s]  It wasn't used very similar once the chargepits here, right? So, I just tried to recognize what was the time when chargepits here at least.  Okay, yeah, chat, you can see.  Okay, now that's all right, 2022. Yeah, so they introduced this per couple of months before chargepits.
[968.40s -> 1005.40s]  And then what happens like, do you remember this movie, sentence floor.  This one. So do you know this movie, so to speak?  It's kind of like a popular from scientific perspective, but they've got the problem, right? So this is the cyber like not that cyberpunk, but probably like interesting science fiction, but take a look at their release years 1999.
[1005.40s -> 1045.40s]  The problem is that in 1999, this movie gets out matrix.  And that's why like everyone know about the matrix, it was like very successful movie.  And this is the reason, much more less people know that this movie, that's for even exists.  If they release it like a year previews before matrix or much more success or the same as we used to write. So chargepits here release November 14 and this is actually the same company, but it was not so hyped.
[1045.40s -> 1075.40s]  In September as it become in November, right?  So what they introduced is they actually have been working like at the same time looks like maybe in working at the visper and parallel like working with chargepits here, right?  So they reused the same like not the same but similar architecture like in quarters decoders and tokens to predict the tokens, but not from text, but from audio as well.
[1075.40s -> 1114.40s]  So they just take a look at the idea, right? And in the paper, so they share like this one.  So they took 600, 80,000 towers of multi-lingual audio.  So they sliced it in 30 seconds, like steps, not so much chunks.  And they, they trained the model, they trained the model against it using the same approach they used for the GPT.
[1114.40s -> 1149.40s]  So pretty the same like sliced data into some samples, tried to predict the tokens like compare with the result, learn,  trained the model and so on. So and this results in a very interesting thing.  Very high quality and I don't know if there are any model right now, which can be it's inequality of visper.  Because once it was released, it was like very high quality they released another model.
[1149.40s -> 1183.40s]  A couple of months ago, I think it's called the visper, but their detection seems to stay the same.  So and one more interesting thing here is it's completely open-served.  So you can download it and it doesn't require a lot of VRAM to run.  So you can run it on the GPU, you can run it a bit slower on the CPU.  And as far as it's like not live language model, it will perform like with acceptable acceptable speed.
[1183.40s -> 1220.40s]  I've been running it at the CPU once and it works normally, so not so slow as the LLM.  So we are not so interested in technical details and I don't understand them actually just to explain to you.  But what we're interested in is actually interested in the result, right? So how it works and how I can use it.  So how how to use it. So you just install it as a Python package, but you need the byte search.  And this is like why I mentioned the Python previously. So in other words, it's a lot of locally.
[1220.40s -> 1250.40s]  So you must have the Python search.  And this is the common bit of like, it should be a Python.  Download probably now. Come on, where's download?  Get started. Oh, here, install.  Okay, like if you would like to try this, right? Or if you would like to work with Python, so there's like two options here.
[1250.40s -> 1280.40s]  So it doesn't matter which version you will be installing. You're going to see the same table for every time like it's a couple of years the same.  So if installing it like in straightforward like recommended way, like usually it's windows, Python, architecture, people, Python and here is like could a OCPU and depending on what you will select.
[1280.40s -> 1311.40s]  The URL will be different, right? So coulda is the accelerated computing framework created by Nvidia as far as remember by Nvidia.  To run the computations on the GPU.  And if you download and install this by touch, you will be running everything against the GPU.  If you do not have GPU, right? So you can install it without like just the CPU.
[1311.40s -> 1347.40s]  And it will be the same package. It will work the same, but much more slower, right?  But it will still work. So you need to prior to install the Python. So first you need to Python. Next you need to select the coulda version. So usually the later the better.  You just copy this and install. And actually I did that today because my Python.  But not Python but visper fate fell to run today. So I've been preparing to the demo and I also have like the.
[1347.40s -> 1377.40s]  Some files.  So I would do files. So and the problem is why.  Facing them and you may face the same. So let me.  Let me show you. So the problem with Python is that.  Somehow like for for for more than applications. I use non-py version to.
[1377.40s -> 1407.40s]  So non-py is a framework is a library to working like with numbers in Python.  And it's required by by touch. But this this by touch and it's like implementation in visper.  And it doesn't work with non-py 2.0.  So it requires like all the version. And that's this kind of problem you may face as well. So what I did is actually I installed.
[1407.40s -> 1439.40s]  Like virtual environments. So I created a virtual Python environment and download it once again.  Pytharch, visper and all the stuff. So at least now it works, right?  So that's that's the problem you may face with working with any software.  In Python. At least I think I should do some kind of work around but.  Taking a look at how many people using the virtual environments. I think this is this is the acceptable work around.  So getting back here. So what you need is just to install the library.
[1439.40s -> 1473.40s]  And that's it. You can install it from source but I prefer like this lazy option.  And it it also requires ffm pack. So ffm pack.  This this is the most popular library to working with audio and video.  So and actually this may be useful for you in future.  So I know like working in development or just processing media like audio and video requires you to do.
[1473.40s -> 1512.40s]  So to convey files from one form to another.  Split them. I don't know change the bit rate and anything. So everything can be done with ffm pack.  But ffm pack is a common line interface.  Like it's a program which allows you to do everything.  But without the user interface, without the UI.  But you can work with it for a bit.  So for example, this is how you can convert from n before to 80 like the old form of right.  And this is like the the greatest software and the defast the solution right now. I think so a lot of.
[1512.40s -> 1547.40s]  Audio and video converting software use ffm pack and that we could.  So it will be necessary as well if you would like to start with this.  Because with this we will need to transform the data from one form or two in other.  So for example, in my example, I'm going to use empathy.  But in some cases, it may be one form or something like that.  So this will need to try to transform from one form or another.  And here are the models and languages. So as far as it's machine learning model right.
[1547.40s -> 1584.40s]  So you can pick up any.  And getting back here. So remember we've been discussing this like model card.  Quantization, distillation and so so different models right. So this is like certain billion model.  This is 70 billion model the same with the speaker.  So this model has 39 million parameters. It's tiny and it requires only one gigabyte of video like.  And it's very fast right. So assuming the large model has like 1.5 billion parameters.
[1584.40s -> 1621.40s]  It'll occupy 10 gigabytes of b-ram. And the speed is 1 like 1 x right.  So this one is 10 times faster and turbo is 8 times faster.  And that's a day they have large width and large width 3.  And what's more interesting here is the quality.  So the quality here is measured in WEAR like world error rate.  So it just evaluating how much how many words like mis-producted like how many words are wrong.
[1621.40s -> 1652.40s]  And you can you can see the languages here. So depending on the data set and probably the language structure.  This really differs right. So I'm not surprised the Spanish is the has like less errors than any other languages because Spanish at least for my perspective I know like a couple of hours.  And it looks like very easily.  Like straight forward and kind of easy.
[1652.40s -> 1693.40s]  And some of the languages probably lacking the presence in the internet will result in like half of the words mis-peeltic beleration.  I don't know. I think I need to I know beleration language and I think I need to find some podcasts in beleration and try to feed it into the whisper and check like how it works.  But this that's someone's strange because 42 out of 100 comparing to six in Ukrainian.  So the lrussian Ukrainian is super similar super similar. So I can understand like 90% of Ukrainian and the crannies understand 90% of beleration probably is the problem in the data set right.
[1693.40s -> 1724.40s]  Okay, but in case like in most cases we're going to work with English right.  So working with this language is super cool.  And common line usage is pretty easy.  So let's run the let's run the example and for the example I have the.  I took the matrix movie and you know like there is a scene.
[1724.40s -> 1760.40s]  I just meet a stock in his morphos.  But which is matrix.  And just me it.  Yes, he's delivering the speech about like the humans.  I'm not sure you will be able to.  To hear it because I'm using teams in in browser. So it's not desktop applications.  So I can't share my audio.  But here is like the the agents meet is interrogating morphos and he's delivering the speech like I got the ideas.
[1760.40s -> 1791.40s]  So human beings are a virus and we like the agents are the cure.  So I will drop you the link or you can just google it.  And and consider this is I picked this this sample because that like high quality speech and no any like music and background and something like that.  So let me try to run this and check how it will work.  So default.
[1791.40s -> 1823.40s]  Whisperer.  We just need to call Whisperer and pass the parameter as the the the data file.  So right now is detecting the language isn't first or the second.  And the the language is detected in which.  So you can do that actually.  Mainly you can specify the language and here is the output right.  So this is like the Smith is fine.
[1823.40s -> 1853.40s]  Discussing it this is very precise actually this is very precise.  I think it's like.  Probably.  Here we have the problems.  Yeah, I think it's messing is messing something still.  But I don't know what model I'm using so let me call the.  The usage.  Okay, so a lot of things here.  So we are interested in the model.
[1853.40s -> 1884.40s]  So we can specify the model from here right.  So these are the available models.  I don't know which one is the default maybe small or base.  So I have a lot of VRAM so I can actually watch.  I'm going to launch large model.  So let's try with probably another model and you can also specify the language explicitly.  I don't think it will help us because it automatically recognize the business English.
[1884.40s -> 1914.40s]  One more thing actually very interesting thing.  You can do with this for so here should be a parameter which is called.  I can find it.  Press.  Information.  Okay, model device output we're both.  The should be parameter which is called something like a prompt.
[1914.40s -> 1947.40s]  Yeah, this one initial prompt.  So the trick is.  So by default it's not but let's get back to the idea so this per.  So this per works with tokens right and everything like in this architecture you can you can pass the system.  So you can pass the system.  And I've been working once on English.  Transcription task.  So we decided to make.
[1947.40s -> 1980.40s]  Transcribed the English assessments in our company so there is like an interview or ask questions and the person replying and one of the idea was to calculate the.  I don't know how it's called like the world's like on when the person is thinking like.  Parasite was.  So it's not possible to do with just plain whisper.  And it's not possible but if you explicitly provide them in a prompt if you recognize them and also.
[1980.40s -> 2012.40s]  A lot of a lot of times in this dialogue.  They were mentioned a company name.  And there's no such world in in whisper data set.  So that's why.  It was looking for a similar.  But if you provide it in initial prompt and it will like see the same talking.  This same talking combination.  It will just do not search for any other alternative. So this is very powerful thing like I'm very underrated.
[2012.40s -> 2048.40s]  Okay, so let's get back here and let's try the model let me try different models and we're going to see.  The performance like this speed.  Let's try with tiny.  This model and let's check how fast until work.  Pretty fast.  And let me try.  Turbo.
[2048.40s -> 2087.40s]  Okay, pretty fast as well.  I don't see a big difference in speed here, right?  At least at this sample the sample is on the one minute.  Okay, yeah, the final talking was took the time and let me try large.  So the large should give us the the highest quality.
[2087.40s -> 2121.40s]  Okay, yeah, this is the difference.  So large and turbo.  There's this tiny so the tiny.  You can just understand what's what's going on in this discussion.  So if you don't care about particular worlds, if you just would like, I don't know,  to summarize it in future and make some decision like I didn't know.  So the sentiment on now is something so maybe tiny is okay, but still.
[2121.40s -> 2153.40s]  That's that's a lot of questions as as far as I see.  But here the turbo and the large so they they work very effectively.  All right, so.  And once I run every model so it's getting downloaded so I don't think I started small.  So probably it will be downloading right now.  No, no, it's already downloaded.  Yeah, so once I selected large first time.
[2153.40s -> 2185.40s]  Yeah, it started downloading all of this model like one and a half gigabytes of model locally.  So if you have a lot of files, I didn't know million of hours.  So what do you just need?  I just need a PC.  So you can use it in cloud, right?  So let's check the pricing open the account.  API pricing.  So you can do that with API.  Let me scroll to the whisper.  By continuing create a system.
[2185.40s -> 2219.40s]  Okay images, audio models.  Okay.  So what we're going to pay.  We're going to pay.  Zero 16 cents per minute.  So let's let me calculate one hour.  Okay.  So zero six cents per minute.  So it means one hour will cost us 36 cents.  Yeah, so it means like 100 hours of transcription will result in 36 dollars.
[2219.40s -> 2250.40s]  So they should be some point where it's cheaper to purchase hardware and run it on your hardware.  And save the hardware for yourself instead of sending to the open.  Yeah, right?  Yeah. So in some cases it's more effective to do it on your on the sheen.  I didn't like spend the night.  Recreate some Python scripts and yeah, regarding Python, you can call it from Python as well.
[2250.40s -> 2282.40s]  So it's Python library.  So you can you can use it from common line, but you can also.  Here is the common time usage.  It can also impart it.  Load the model and transcribe your own data just as passing as reference to the.  To the file right and here are the examples how we can do that with.  We spite them still it has one problem, which probably being solved in some forks of the whisper like whisper X and so on.
[2282.40s -> 2313.40s]  The problem is it doesn't differentiate speakers so you will not see the difference between speaker one and speaker to write.  So what I did in homework, I shared the file with you where there is a difference between speaker one.  And speaker so here is no no any difference, but the output form us may be different right so.  Let me call the help once again.
[2313.40s -> 2345.40s]  Here output format you can see we have a lot of different output form was TXT with TSNT TSVG.  So all let's try let's try to run with all but probably all is a default right so let me share.  Okay yeah all is a default it means I already have all this information available.  So let's let's check it out.  If you like json's you can run this.
[2345.40s -> 2349.40s]  It would be for my good right.
